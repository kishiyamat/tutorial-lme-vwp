---
layout: post
title: 一般化線形混合モデル
description: LMEの解説ページ
---

線形モデルを一般化させてランダム効果を考えることで

先ほどの式を $y = a_1x_1$ のように簡略するとして、
この $a_1$ が $y = a_1x_1 + a_{subject_i}x_{subject_i}$ 
のようになっていると考えます。
つまり傾き$a$ は真の傾き$a_{true}$ に個人のノイズ $a_{subject_i}$ が
足されたものだよ、と表現してあげるわけです。
これをRというプログラミング言語で表現すると
(複雑さを避けるため、一旦要因が $x_1$ しかないケースを考える)、
`y ~ x_1 + (x_1 | subject)` と表現します。
違和感があると思うので一つずつ解消していきましょう。

まず「あれ、$a$とか$b$とかどこいったの」となるかもしれません。
ただこれは単純な話で、$a$ の値がどうなっているのを知りたいということは
$a$ が未知なものということですよね。
Rではこういう未知なものを参照できないのです。
対照的に、`y`は観測した読み時間だし `x_1` はこちらが操作したであろう要因なので
既知ですよね。また、個人を示す `subject` も既知です。
したがって不明な$a$などのパラメータは記述せず、
既知なものだけでデータの関係を記述します。

つぎに「`(x_1 | subject)` って何」という話ですが、
個人`subject` ごとの`x_1` もモデルに組み込むことを示しています。
これにも対する傾きも推定することになりので、その傾きが先ほどの
$a = a_{true} + a_{subject_i}$ に出てきた $a_{subject_i}$ になります。
よく条件付けはバー、$|$で表しま すが、この条件付けられた `x_1` に対する傾きが
構造的なノイズになります。
こういう奴もモデリングしてあげて、後で無視するわけです。
<!--
さっきとは式自体は違いますが結果は同じことを余裕があれば確認してください。
-->
なお「あれ$b$はどこ」という点については
デフォルトでRは式に`1`を追加していてくれるので、
それに対する傾きも未知となります。

先ほど「一般化線形混合モデル」は
構造的なノイズを「ランダム効果」として
切り捨てて本当に見たい効果が見れる、という話をしました。
そしてこのランダム効果は `y ~ x_1 + (x_1 | subject)` という形で
表現できることも述べました。
